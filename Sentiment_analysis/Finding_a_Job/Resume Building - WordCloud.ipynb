{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:55:07.041574Z",
     "start_time": "2025-04-13T11:55:07.035956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tarfile import version\n",
    "\n",
    "import streamlit as st\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ],
   "id": "cbeecb6e51b16845",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jahdovanterpool/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jahdovanterpool/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:55:07.054752Z",
     "start_time": "2025-04-13T11:55:07.051562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set stop words to a variable\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ],
   "id": "3a4c41025883e360",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "198\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:55:07.075832Z",
     "start_time": "2025-04-13T11:55:07.072281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z]\\s\", \" \", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    words = word_tokenize(text)\n",
    "    word = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(word)\n",
    "\n",
    "def get_top_keywords(text, num=10):\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return Counter(words).most_common(num)\n",
    "\n",
    "def categorize_keywords(keywords, technical_skills, soft_skills):\n",
    "    tech = [word for word in keywords if word in technical_skills]\n",
    "    soft = [word for word in keywords if word in soft_skills]\n",
    "    other = [word for word in keywords if word not in soft_skills and word not in technical_skills]\n",
    "    return tech, soft, other\n",
    "\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.title(title)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    #st.pyplot(plt)"
   ],
   "id": "f724c566b3fda94a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Testing",
   "id": "3293cbe12a9510d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:55:07.091324Z",
     "start_time": "2025-04-13T11:55:07.086733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_description = \"\"\"\n",
    "Put your career in motion with a great opportunity. Work as a Smartsheet Data Analyst at GE.\n",
    "Import and clean data from Excel and Smartsheet, create dashboards, collaborate with teams.\n",
    "\"\"\"\n",
    "\n",
    "resume_text = \"\"\"\n",
    "Experienced in Excel, Python, and data reporting. Collaborated with cross-functional teams and automated dashboards.\n",
    "\"\"\"\n",
    "\n",
    "tech_keywords = {'excel', 'python', 'sql', 'smartsheet', 'data', 'automation', 'dashboards'}\n",
    "soft_keywords = {'collaboration', 'communication', 'team', 'leadership'}\n",
    "\n",
    "# Clean and process\n",
    "job_clean = clean_text(job_description)\n",
    "resume_clean = clean_text(resume_text)\n",
    "\n",
    "# Get top keywords\n",
    "job_kw = [word for word, _ in get_top_keywords(job_clean)]\n",
    "resume_kw = [word for word, _ in get_top_keywords(resume_clean)]\n",
    "\n",
    "# Compare\n",
    "job_set = set(job_kw)\n",
    "resume_set = set(resume_kw)\n",
    "\n",
    "missing = job_set - resume_set\n",
    "match = job_set & resume_set\n",
    "\n",
    "print(\"Matching Keywords:\", match)\n",
    "print(\"Missing from Resume:\", missing)\n",
    "\n",
    "# Categorize\n",
    "tech, soft, other = categorize_keywords(job_kw, tech_keywords, soft_keywords)\n",
    "print(\"\\nTech Skills:\", tech)\n",
    "print(\"Soft Skills:\", soft)\n",
    "print(\"Other:\", other)\n"
   ],
   "id": "22df7aa3b90785bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching Keywords: {'data'}\n",
      "Missing from Resume: {'great', 'motion', 'ge', 'analyst', 'career', 'smartsheet', 'work', 'opportunity', 'put'}\n",
      "\n",
      "Tech Skills: ['smartsheet', 'data']\n",
      "Soft Skills: []\n",
      "Other: ['put', 'career', 'motion', 'great', 'opportunity', 'work', 'analyst', 'ge']\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:55:07.189993Z",
     "start_time": "2025-04-13T11:55:07.097598Z"
    }
   },
   "cell_type": "code",
   "source": "generate_wordcloud(resume_text,title = 'test')",
   "id": "768c6588af71de83",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:55:07.211738Z",
     "start_time": "2025-04-13T11:55:07.208996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "string1 = set(['i','love','you'])\n",
    "string2 = set(['i'])\n",
    "\n",
    "string1 - string2"
   ],
   "id": "d00ae42c3231e08b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love', 'you'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:11:58.081350Z",
     "start_time": "2025-04-13T12:11:58.071838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Set up\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Define your helper functions\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    tagged = pos_tag(tokens)\n",
    "    filtered = [lemmatizer.lemmatize(word) for word,tag in tagged if tag.startswith('N') or tag.startswith('V')]\n",
    "\n",
    "    bigrams = [\" \".join(gram) for gram in ngrams(filtered, 2)]\n",
    "    trigrams = [\" \".join(gram) for gram in ngrams(filtered, 3)]\n",
    "\n",
    "    return filtered + bigrams + trigrams\n",
    "def get_top_keywords(tokens, num=50):\n",
    "    return Counter(tokens).most_common(num)\n",
    "\n",
    "# Example job description\n",
    "job_description = \"\"\"\n",
    "We are looking for a data analyst with strong communication and leadership skills.\n",
    "The candidate must have experience with Python, Excel, and data visualization tools.\n",
    "\"\"\"\n",
    "\n",
    "# Clean and process\n",
    "cleaned = clean_text(job_description)\n",
    "job_tokens = tokenize_and_filter(cleaned)\n",
    "jop_top = get_top_keywords(job_tokens)\n"
   ],
   "id": "875bf74a971eb918",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jahdovanterpool/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jahdovanterpool/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jahdovanterpool/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jahdovanterpool/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jahdovanterpool/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T12:12:03.327630Z",
     "start_time": "2025-04-13T12:12:03.325863Z"
    }
   },
   "cell_type": "code",
   "source": "print(jop_top)",
   "id": "75a476005205fe7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('data', 2), ('looking', 1), ('analyst', 1), ('communication', 1), ('leadership', 1), ('skill', 1), ('candidate', 1), ('experience', 1), ('excel', 1), ('visualization', 1), ('tool', 1), ('looking data', 1), ('data analyst', 1), ('analyst communication', 1), ('communication leadership', 1), ('leadership skill', 1), ('skill candidate', 1), ('candidate experience', 1), ('experience excel', 1), ('excel data', 1), ('data visualization', 1), ('visualization tool', 1), ('looking data analyst', 1), ('data analyst communication', 1), ('analyst communication leadership', 1), ('communication leadership skill', 1), ('leadership skill candidate', 1), ('skill candidate experience', 1), ('candidate experience excel', 1), ('experience excel data', 1), ('excel data visualization', 1), ('data visualization tool', 1)]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e15c92bd614d2c7c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
